{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "D9iIjZvZeEEo",
        "E3RnfVUceRM5",
        "akGK5YpRjcu_"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WSH032/ChatGLM-webui/blob/main/Colab_ChatGLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "52QXFL73_0Pl"
      },
      "outputs": [],
      "source": [
        "#@title #ï¼ˆä¸€ï¼‰å…‹éš†ç§‹å¶çš„åº“ã€å®‰è£…ä¾èµ–\n",
        "\n",
        "!git clone https://github.com/Akegarasu/ChatGLM-webui\n",
        "%cd /content/ChatGLM-webui\n",
        "print(f\"æ­£åœ¨å®‰è£…ä¾èµ–ï¼Œè¯·è€å¿ƒç­‰å¾…\")\n",
        "!pip install --upgrade -r requirements.txt  > /dev/null 2>&1\n",
        "print(f\"ä¾èµ–å®‰è£…å®Œæˆ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ï¼ˆäºŒï¼‰é€‰æ‹©ä¸€ä¸ªæ–¹æ³•å¹¶ä½¿ç”¨"
      ],
      "metadata": {
        "id": "noglVVk7d7xN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1ç§‹å¶æ–¹æ³•ï¼ˆWebUIï¼‰"
      ],
      "metadata": {
        "id": "D9iIjZvZeEEo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ###2.1.1 è®¾ç½®å‚æ•°å¹¶å¼€å§‹å¯¹è¯\n",
        "\n",
        "extArgs=\"\"\n",
        "\n",
        "#@markdown æ˜¯å¦ç”¨CPUè¿›è¡Œæ¨ç†`CPUæ¨¡å¼è¯·é€‰æ‹©chatglm-6b-int4-qeï¼Œä¸ç„¶ä¼šçˆ†ram`\n",
        "use_cpu = False #@param {type:\"boolean\"}\n",
        "#å¯ç”¨CPUæ¨¡å‹\n",
        "if use_cpu:\n",
        "  extArgs = extArgs + \"--cpu \"\n",
        "\n",
        "#@markdown é€‰æ‹©æ¨¡å‹`Colabå…è´¹ç”¨æˆ·åªèƒ½ä½¿ç”¨int4å’Œqeæ¨¡å‹`ï¼Œæˆ–è€…å¡«å…¥è‡ªå®šä¹‰æ¨¡å‹è·¯å¾„`å°†ä¼šè¦†ç›–é¢„è®¾æ¨¡å‹é€‰æ‹©`\n",
        "model_path = \"THUDM/chatglm-6b-int4\" #@param [\"THUDM/chatglm-6b\", \"THUDM/chatglm-6b-int4\", \"THUDM/chatglm-6b-int4-qe\"]\n",
        "your_model_path = \"\" #@param {type:\"string\"}\n",
        "#ç”¨è‡ªå®šä¹‰è·¯å¾„è¦†ç›–é¢„è®¾\n",
        "if your_model_path:\n",
        "  model_path = your_model_path\n",
        "\n",
        "#@markdown æ¨ç†ç²¾åº¦`ç•™ç©ºåˆ™è‡ªåŠ¨æŒ‡å®š, fp32åªæœ‰CPUå¯ä½¿ç”¨ ï¼› int4ã€int8åªæœ‰GPUèƒ½ç”¨`\n",
        "precision = \"\" #@param [\"\", \"fp32\", \"fp16\", \"int4\", \"int8\"]\n",
        "#æŒ‡å®šç²¾åº¦\n",
        "if precision:\n",
        "  extArgs = extArgs + f\"--precision={precision} \"\n",
        "\n",
        "#å¯åŠ¨\n",
        "!python webui.py --model-path={model_path} --listen --share {extArgs}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "jIiNeWyKAL6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2WSHæ–¹æ³•ï¼ˆColabé£æ ¼`æ”¯æŒæµå¼å¯¹è¯`ï¼‰"
      ],
      "metadata": {
        "id": "E3RnfVUceRM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ###2.2.1é€‰æ‹©å¹¶å¯ç”¨æ¨¡å‹\n",
        "\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import gradio as gr\n",
        "\n",
        "#@markdown æ˜¯å¦ç”¨CPUè¿›è¡Œæ¨ç†`CPUæ¨¡å¼è¯·é€‰æ‹©chatglm-6b-int4-qeï¼Œä¸ç„¶ä¼šçˆ†ram`\n",
        "use_cpu = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown é€‰æ‹©æ¨¡å‹`Colabå…è´¹ç”¨æˆ·åªèƒ½ä½¿ç”¨int4å’Œqeæ¨¡å‹`ï¼Œæˆ–è€…å¡«å…¥è‡ªå®šä¹‰æ¨¡å‹è·¯å¾„`å°†ä¼šè¦†ç›–é¢„è®¾æ¨¡å‹é€‰æ‹©`\n",
        "model_path = \"THUDM/chatglm-6b-int4\" #@param [\"THUDM/chatglm-6b\", \"THUDM/chatglm-6b-int4\", \"THUDM/chatglm-6b-int4-qe\"]\n",
        "your_model_path = \"\" #@param {type:\"string\"}\n",
        "#ç”¨è‡ªå®šä¹‰è·¯å¾„è¦†ç›–é¢„è®¾\n",
        "if your_model_path:\n",
        "  model_path = your_model_path\n",
        "\n",
        "if use_cpu:\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
        "  model = AutoModel.from_pretrained(model_path, trust_remote_code=True).float()\n",
        "else:\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
        "  model = AutoModel.from_pretrained(model_path, trust_remote_code=True).half().cuda()\n",
        "\n",
        "model = model.eval()\n",
        "\n",
        "#åˆå§‹åŒ–å‚æ•°\n",
        "history = []\n",
        "count = 1\n",
        "max_length = 2048\n",
        "top_p = 0.7\n",
        "temperature =  0.95\n",
        "max_turns = 20\n",
        "clear_history_flag = False\n",
        "\n",
        "def clear_history_set():\n",
        "  global history, count, clear_history_flag\n",
        "  history = []\n",
        "  count = 1\n",
        "  clear_history_flag = False"
      ],
      "metadata": {
        "cellView": "form",
        "id": "m_FzCEwuUvKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ###2.2.2å‚æ•°è®¾ç½®ï¼ˆå¯¹è¯ä¸­ä¹Ÿå¯æ›´æ”¹ï¼‰\n",
        "max_length = 2048 #@param {type:\"number\"}\n",
        "top_p = 0.7 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "temperature =  0.95 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "max_turns = 20 #@param {type:\"slider\", min:1, max:256, step:1}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "pqaSs3MgbnV4"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ###2.2.3æé—®\n",
        "\n",
        "if clear_history_flag:\n",
        "  clear_history_set()\n",
        "  print(f\"è¾¾åˆ°å¯¹è¯æ¬¡æ•°ä¸Šé™ï¼Œå†å²å¯¹è¯è®°å½•å·²è¢«æ¸…ç©º\")\n",
        "\n",
        "def ask_and_ans(tokenizer, ask, history, max_length, top_p, temperature):\n",
        "  old_history = \"\"\n",
        "  old_response = \"\"\n",
        "  for response, history in model.stream_chat(tokenizer, ask, history, max_length, top_p, temperature):\n",
        "    old_response = response\n",
        "    old_history = history\n",
        "    print(end=\"\\r\")\n",
        "    print(response, end=\"\", flush=True) # æ‰“å°å½“å‰å­—ç¬¦ä¸²\n",
        "  print(end=\"\\r\")\n",
        "  print(old_response)\n",
        "  return old_response, old_history\n",
        "\n",
        "print(f\"ç¬¬ {count} æ¬¡å¯¹è¯ï¼Œåˆ°è¾¾ {max_turns} æ¬¡åï¼Œä¸‹ä¸€æ¬¡å¯¹è¯æ—¶å°†åˆ é™¤å†å²å¯¹è¯è®°å½•\")\n",
        "\n",
        "ask = \"\" #@param {type:\"string\"}\n",
        "print(\"\\n\")\n",
        "response, history = ask_and_ans(tokenizer, ask, history, max_length=max_length, top_p=top_p,temperature=temperature)\n",
        "count = count +1\n",
        "\n",
        "if count > max_turns:\n",
        "  clear_history_flag = True\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "XYEVkeAqduqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ###2.2.4 å†å²å¯¹è¯è®°å½•\n",
        "\n",
        "\n",
        "\n",
        "#@markdown æ˜¾ç¤ºå¯¹è¯å†å²\n",
        "show_history = True #@param {type:\"boolean\"}\n",
        "if show_history:\n",
        "  for ask_contet,ans_content in history:\n",
        "    print(f\"ç”¨æˆ·ï¼š {ask_contet}\")\n",
        "    print(f\"å›ç­”ï¼š {ans_content}\")\n",
        "    print(f\"------------------------------------------------------\")\n",
        "#@markdown æ‰‹åŠ¨æ¸…ç©ºå¯¹è¯å†å²\n",
        "clear_history = False #@param {type:\"boolean\"}\n",
        "if clear_history:\n",
        "  clear_history_set()\n",
        "  print(f\"èŠå¤©è®°å½•å·²è¢«æ¸…ç©º\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "kTAUdZZ1kniH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3å®˜æ–¹æµå¼æ–¹æ³•"
      ],
      "metadata": {
        "id": "akGK5YpRjcu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ###2.3.1é€‰æ‹©å¹¶å¯ç”¨æ¨¡å‹\n",
        "\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import gradio as gr\n",
        "\n",
        "#@markdown æ˜¯å¦ç”¨CPUè¿›è¡Œæ¨ç†`CPUæ¨¡å¼è¯·é€‰æ‹©chatglm-6b-int4-qeï¼Œä¸ç„¶ä¼šçˆ†ram`\n",
        "use_cpu = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown é€‰æ‹©æ¨¡å‹`Colabå…è´¹ç”¨æˆ·åªèƒ½ä½¿ç”¨int4å’Œqeæ¨¡å‹`ï¼Œæˆ–è€…å¡«å…¥è‡ªå®šä¹‰æ¨¡å‹è·¯å¾„`å°†ä¼šè¦†ç›–é¢„è®¾æ¨¡å‹é€‰æ‹©`\n",
        "model_path = \"THUDM/chatglm-6b-int4\" #@param [\"THUDM/chatglm-6b\", \"THUDM/chatglm-6b-int4\", \"THUDM/chatglm-6b-int4-qe\"]\n",
        "your_model_path = \"\" #@param {type:\"string\"}\n",
        "#ç”¨è‡ªå®šä¹‰è·¯å¾„è¦†ç›–é¢„è®¾\n",
        "if your_model_path:\n",
        "  model_path = your_model_path\n",
        "\n",
        "if use_cpu:\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
        "  model = AutoModel.from_pretrained(model_path, trust_remote_code=True).float()\n",
        "else:\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
        "  model = AutoModel.from_pretrained(model_path, trust_remote_code=True).half().cuda()\n",
        "\n",
        "model = model.eval()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "mRlNHn6FmATu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ###2.3.2å¼€å¯å¯¹è¯\n",
        "\n",
        "#@markdown æœ€å¤§å¯¹è¯è½®æ•°\n",
        "MAX_TURNS = 20 #@param {type:\"slider\", min:1, max:256, step:1}\n",
        "MAX_BOXES = MAX_TURNS * 2\n",
        "\n",
        "\n",
        "\n",
        "def predict(input, max_length, top_p, temperature, history=None):\n",
        "    if history is None:\n",
        "        history = []\n",
        "    for response, history in model.stream_chat(tokenizer, input, history, max_length=max_length, top_p=top_p,\n",
        "                                               temperature=temperature):\n",
        "        updates = []\n",
        "        for query, response in history:\n",
        "            updates.append(gr.update(visible=True, value=\"ç”¨æˆ·ï¼š\" + query))\n",
        "            updates.append(gr.update(visible=True, value=\"ChatGLM-6Bï¼š\" + response))\n",
        "        if len(updates) < MAX_BOXES:\n",
        "            updates = updates + [gr.Textbox.update(visible=False)] * (MAX_BOXES - len(updates))\n",
        "        yield [history] + updates\n",
        "\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    state = gr.State([])\n",
        "    text_boxes = []\n",
        "    for i in range(MAX_BOXES):\n",
        "        if i % 2 == 0:\n",
        "            text_boxes.append(gr.Markdown(visible=False, label=\"æé—®ï¼š\"))\n",
        "        else:\n",
        "            text_boxes.append(gr.Markdown(visible=False, label=\"å›å¤ï¼š\"))\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=4):\n",
        "            txt = gr.Textbox(show_label=False, placeholder=\"Enter text and press enter\", lines=11).style(\n",
        "                container=False)\n",
        "        with gr.Column(scale=1):\n",
        "            max_length = gr.Slider(0, 4096, value=2048, step=1.0, label=\"Maximum length\", interactive=True)\n",
        "            top_p = gr.Slider(0, 1, value=0.7, step=0.01, label=\"Top P\", interactive=True)\n",
        "            temperature = gr.Slider(0, 1, value=0.95, step=0.01, label=\"Temperature\", interactive=True)\n",
        "            button = gr.Button(\"Generate\")\n",
        "    button.click(predict, [txt, max_length, top_p, temperature, state], [state] + text_boxes)\n",
        "demo.queue().launch()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "nBSLEcXyjoBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *ï¼ˆä¸‰ï¼‰å¼€å‘å¤‡ç”¨ä»£ç *"
      ],
      "metadata": {
        "id": "HSWWiFDvjBn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "old_response = \"\"\n",
        "for response, history in model.stream_chat(tokenizer, \"ä½ å¥½\", [], max_length=2048, top_p=0.7, temperature=0.95):\n",
        "  print(response[len(old_response):], end=\"\")\n",
        "  old_response = response\n",
        "print(end=\"\\r\")\n",
        "print(old_response)"
      ],
      "metadata": {
        "id": "5jh610GcQe5m",
        "outputId": "14bb8813-cea7-46af-be74-313a5023ac17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "old_response = \"\"\n",
        "for response, history in model.stream_chat(tokenizer, \"ä½ å¥½\", [], max_length=2048, top_p=0.7, temperature=0.95):\n",
        "  old_response = response\n",
        "  sys.stdout.write(response)\n",
        "  sys.stdout.flush()\n",
        "  sys.stdout.write(\"\\r\")\n",
        "print(old_response) #"
      ],
      "metadata": {
        "id": "Rtju7dXTRc9E",
        "outputId": "bbdee23d-00b5-481d-dd28-15875f8bdd38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚\n"
          ]
        }
      ]
    }
  ]
}